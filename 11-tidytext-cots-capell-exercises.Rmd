---
title: "11-tidytext - exercises"
author: "J√∫lia Cots-Capell & Viktorija Ruzelyte"
date: "15th November 2022"
output:
  html_document:
    code_folding: show
    df_print: paged
    highlight: tango
    number_sections: no
    theme: cosmo
    toc: no
    keep_md: true
---
  
<style>
div.answer {background-color:#f3f0ff; border-radius: 5px; padding: 20px;}
</style>

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      eval = TRUE,
                      error = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      comment = NA)
```

```{r, include = T}
library(rvest)
library(stringr)
library(tidyverse)
library(tidytext)
library(wordcloud2)
library(ggplot2)
```

<br>

#### üëã **WELCOME TO WORKSHOP #11 ON THE PACKAGE TIDYTEXT**

In this workshop we're going to discover the utilities of the R package "tidytext", created by Julia Silge and David Robinson. 

For the purpose of having a practical understanding of the package, we're going to use two of the most famous political speeches of the XXI century in the United States: 

- President Obama's Inaugural Address; 20th January 2009. 

- President Trump's Inaugural Address; 20th January 2017. 

<br>

#### üë©üíª **STEP 0. Data scraping and preparation.**

In order to obtain the inauguration speeches that we're interested in analysing, we first need to scrape the websites with the transcripts of the speeches. For the purpose of this workshop, and in the interest of time, we provide you with the code to obtain the text. Each president's speech is stored in a string named "*_text". 

```{r}
#OBAMA
obama_link <- read_html("http://obamaspeeches.com/P-Obama-Inaugural-Speech-Inauguration.htm")
obama_text <- html_text(html_elements(obama_link, xpath = "//td/font[@size='3']")) 

#TRUMP
trump_link <- read_html("https://www.politico.com/story/2017/01/full-text-donald-trump-inauguration-speech-transcript-233907")
trump_text <- html_text(html_elements(trump_link, xpath = "//div[@class='story-text']/p"))
trump_text <- trump_text[c(2:14)]

```


We will now convert the strings into dataframes for better manipulation in the future steps and in order to be able to apply the tidytext package functions which require a tidy-data structure.

``` {r}
obama_speech_df <- tibble(Text=obama_text) #here we obtain a 1x1 dataframe, given that the text is stored in 1 single character block
trump_speech_df <- tibble(Text=trump_text) #here we obtain a 13x1 dataframe, given that the text is stored in 13 paragraphs

```

We can see that each speech is stored as rows of the dataframe, with the whole text stored in **one column**. However, because of the formatting of every website from which we obtained the transcript, Obama's speech is stored as one single row, whereas Trump's speech is split in paragraphs. 

Nevertheless, our future analysis won't be affected by this circumstance, given that our unit of interest will be the individual words of the speeches. 

<br>

#### üßπüßº **STEP 1. Clean the data (tidy data).**

Once we have saved the text in the corresponding dataframes, we will start the actual work. The tidy text format is a table with one token per row. Tokens are the unit of text that we are interested in. In our case, our tokens are **words**. 

**Task 1.** *Convert the dataframes into tidy text format, with words as tokens. Print the first 10 lines of each new dataframe to make sure that you're doing it alright.*    
````{r}
#YOUR CODE HERE

````
<br>

**Task 2.** *To be able to compare across speeches, we  want to have the two columns in a single dataframe. Construct a dataframe with all the words used in both speeches, with the corresponding indication of the president who said every word.*
```{r}
#YOUR CODE HERE

```


<br>

#### üóØüì£ **STEP 2. Word analysis.**

The tidytext package has some functionalities that are very useful to analyse the meaningful content of the text. 

**Task 3.** *Create a new dataframe of the speeches excluding English stopwords.*

```{r}
#YOUR CODE HERE

```

<br>

Having text in tidy format also allows us to analyse which words are used most frequently in different texts and then compare them. In the following exercises we will practise this functionality. 

**Task 4.** *Identify the 5 most common words (excluding stopwords) used by each president in their inauguration speeches.*
````{r}
#YOUR CODE HERE

````
<br>

**Task 5.** *Identify the words that at least one of the two presidents used more than 1% of the times.*
```{r}
#YOUR CODE HERE

```

<br/>

#### üòç/üò¢ **STEP 3. Sentiment analysis.**

The tidytext package also allows us to do opinion mining or sentiment analysis. Tidytext follows the same process that the human brain does: when we read a text, we use our understanding of the emotional direction or intent of the words to infer whether the paragraph, section, chapter or book is positive or negative. 

Text mining tools will help us approach the emotional content of the two speeches, as shown in the image below: 

```{r, fig.align='center', echo=F, out.width = "90%"}

knitr::include_graphics("../flowchart-sentiment_analysis.png")
```

However, sentiments are subjective, and there is more than one methods and dictionary for evaluating the opinion or the emotions in the text. Tidytext provides access to several **sentiment lexicons**, such as the following: 

- *AFINN*, from Finn √Örup Nielsen.
- *bing*, from Bing Liu and collaborators.
- *nrc*, from Saif Mohammad and Peter Turney.

The function **get_sentiments("name of the package")** allows us to get specific sentiment lexicons with the appropriate measures for each one. For this exercise we're going to use the *nrc* package. 

<br>

**Task 6.** *Analyse the general sentiment of the speeches. Which president used a higher share of negative words?*

````{r}
#YOUR CODE HERE

````

<br>

#### üìä **STEP 4. Plot the results.** 

Finally we can plot our results (from the word frequency or sentiment analysis). To do so, we can use packages such as dplyr and ggplot. 
<br>

**Task 7.** *Visualise in a plot the 6-most used positive and negative words in the two speeches*
```{r}
#YOUR CODE HERE

```


However, in text analysis we can also use an even cooler package that generates word clouds. This package is called *wordcloud2*, and we can use it inside the dplyr functionalities following the same structure as ggplot. It prints an interactive cloud of words which also tells us the number of times each word appears. <br>
To do so, you can use the dplyr functionalities with the function **wordcloud2(size = , color = c())**. 
<br>

**Task 8.** *Print the 50 words that Obama used the most in a wordcloud and highlight in three different colours the top-3.*
```{r}
#YOUR CODE HERE

```


<br>

‚ûïü§ì **Additional exercises and material**

In some cases it might be useful to investigate specific words in the text. In the case of the two inauguration speeches, we may wonder whether either Obama or Trump would have said words like 'I' or 'we' in a higher share. We believe that the usage of this words might reflect a more selfish attitude, and we want to investigate who said either of the two words more. <br>
To do so, we will need to first count the number of times each president said each of these words, and later calculate the proportion among the whole speech. 
<br>
*Note.* Be careful in chosing a data set. 'I' and 'we' are considered stop words and so we need the data that contains them. 
<br>

**Additional task.** *How often did both presidents use words "I" and "we" in their speeches?*

````{r}
#YOUR CODE HERE

````

<br>
**Additional materials**. <br>
For further learning, you can check the following materials: <br>
- Julia Silge, one of the authors of the package has a video in Youtube explaining their work through the "janeausten" text, https://www.youtube.com/watch?v=0poJP8WQxew. <br>
- Dave Robinson, the other author of the package, used the package to analyse Donald Trump's tweets, http://varianceexplained.org/r/trump-tweets/.

<br>

üìö **REFERENCES**

***R-packages***: 
  <br>
Silge J, Robinson D (2016). ‚Äútidytext: Text Mining and Analysis Using Tidy Data Principles in R.‚Äù _JOSS_,
  *1*(3). doi:10.21105/joss.00037 <https://doi.org/10.21105/joss.00037>,
  <http://dx.doi.org/10.21105/joss.00037>.
  <br>
Wickham H (2022). _stringr: Simple, Consistent Wrappers for Common String Operations_. R package version
  1.4.1, <https://CRAN.R-project.org/package=stringr>.
    <br>
Wickham H. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.
  <br>
Wickham H (2022). _rvest: Easily Harvest (Scrape) Web Pages_. R package version 1.0.3,
  <https://CRAN.R-project.org/package=rvest>.
  <br>
Wickham H, Averick M, Bryan J, Chang W, McGowan LD, Fran√ßois R, Grolemund G, Hayes A, Henry L, Hester J,
  Kuhn M, Pedersen TL, Miller E, Bache SM, M√ºller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K,
  Vaughan D, Wilke C, Woo K, Yutani H (2019). ‚ÄúWelcome to the tidyverse.‚Äù _Journal of Open Source
  Software_, *4*(43), 1686. doi:10.21105/joss.01686 <https://doi.org/10.21105/joss.01686>.
  <br>
Wickham H, Fran√ßois R, Henry L, M√ºller K (2022). _dplyr: A Grammar of Data Manipulation_. R package
  version 1.0.10, <https://CRAN.R-project.org/package=dplyr>.

<br>
***Bibliography***
<br>
Robinson, Julia Silge and David. Introduction to Tidytext, 19 Aug. 2022, https://cran.rproject.org/web/packages/tidytext/vignettes/tidytext.html.
<br>
Robinson, Julia Silge and David. ‚ÄúWelcome to Text Mining with r: Text Mining with R.‚Äù Welcome to Text Mining with R | Text Mining with R, https://www.tidytextmining.com/.
<br>
‚ÄúTidy Text.‚Äù YouTube, YouTube, 1 Mar. 2022, https://www.youtube.com/watch?v=Udp2WlvuWHo&amp;t=645s.
<br>
‚ÄúText Mining, the Tidy Way.‚Äù YouTube, YouTube, 16 May 2017, https://www.youtube.com/watch?v=0poJP8WQxew&amp;t=400s. 
